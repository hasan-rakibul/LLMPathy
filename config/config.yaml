expt_name: LLMPathy_1GPU
seed: 42
num_workers: 12
cuda_visible_devices: 2, 6

checkpoint: roberta-base
# checkpoint: mistralai/Mistral-7B-v0.1

data:
  train_file: ./data/v2_v3_train_augmented.tsv
  val_file: ./data/v2_dev.tsv
  feature_to_tokenise: ['demographic_essay']
  max_length: 512
  demographics: []
  # demographics: ['gender', 'education', 'race', 'age', 'income']
  ssl_threshold: 0.5

train:
  num_epochs: 50
  output_dir: ./output
  logging_dir: ./logs
  lr: 1.0e-4
  lr_scheduler_type: constant
  lr_step_period: 10
  warmup_ratio: 0.1
  max_grad_norm: 0.3
  batch_size: 32
  weight_decay: 0.001
  fp16: False
  gradient_checkpointing: True
  early_stop: #TODO: implement early stopping
    patience: 5
    delta: 0.0
  dropout: 0.2
