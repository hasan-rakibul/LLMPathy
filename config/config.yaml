expt_name: LLMPathy
seed: 42
num_workers: 12
checkpoint: roberta-base
# checkpoint: 

data:
  train_file: ./data/v2_v3_train_augmented.tsv
  val_file: ./data/v2_dev.tsv
  annotation: ['crowdsourced_empathy']
  # annotation: ['crowdsourced_empathy', 'gpt_empathy']
  feature_to_tokenise: ['demographic_essay']
  max_length: 512
  demographics: []
  # demographics: ['gender', 'education', 'race', 'age', 'income']

train:
  output_dir: ./output
  logging_dir: ./logs
  lr: 1.0e-4
  lr_scheduler_type: constant
  warmup_ratio: 0.1
  max_grad_norm: 0.3
  num_train_epochs: 5
  batch_size: 32
  weight_decay: 0.001
  fp16: False
  gradient_checkpointing: True
  early_stop: #TODO: implement early stopping
    patience: 5
    delta: 0.0