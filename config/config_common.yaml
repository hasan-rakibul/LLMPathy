seeds: [0, 42, 100, 999, 1234]
num_workers: 12
plm: roberta-base
# plm: siebert/sentiment-roberta-large-english

num_epochs: 10
lr: 2e-05
batch_size: 16
max_length: 512

logging_dir: ./logs
# fc_arch: [768, 512, 256, 1] # [out_transforer, out_pre_fusion, out_fusion, out]
freeze_plm: False
lr_find: False  # unfornately, it didn't work for me

adamw_beta1: 0.9
adamw_beta2: 0.98
adamw_eps: 1e-06
adamw_weight_decay: 0.1

lr_scheduler_type: linear
linear_warmup: 0.06
# linear_num_warmup_steps: 300

# lr_scheduler_type: plateau
# plateau_patience: 10
# plateau_factor: 0.676
# plateau_threshold: 0.0057

label_column: empathy
# llm_column: llm_empathy
feature_to_tokenise: ["essay"]
# feature_to_tokenise: ["demographic_essay"]
# extra_columns_to_keep: ["article_id"]
extra_columns_to_keep: []

### extra column in train only
# extra_columns_to_keep_train: []
extra_columns_to_keep_train: ["llm_empathy"]

# demographics: ['gender', 'education', 'race', 'age', 'income']
# demographics_2024: ['person_id']
