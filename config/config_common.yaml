seed: 0
num_workers: 12
plm: roberta-base
num_epochs: 20
lr: 3.8e-5
batch_size: 2
logging_dir: ./logs
# fc_arch: [768, 512, 256, 1] # [out_transforer, out_pre_fusion, out_fusion, out]
max_length: 512
freeze_plm: False
lr_find: False  # unfornately, it didn't work for me

adamw_beta1: 0.88
adamw_beta2: 0.98
adamw_eps: 4.0e-7
adamw_weight_decay: 0.035

# lr_scheduler_type: linear
# linear_warmup: 0.06

lr_scheduler_type: plateau
plateau_patience: 7
plateau_factor: 0.326
plateau_threshold: 0.0075

# alpha: 3.5

label_column: empathy
# llm_column: llm_empathy
feature_to_tokenise: ["essay"]
# feature_to_tokenise: ["demographic_essay"]
# extra_columns_to_keep: ["article_id"]
extra_columns_to_keep: []

### extra column in train only
# extra_columns_to_keep_train: []
extra_columns_to_keep_train: ["llm_empathy"]


# demographics: ['gender', 'education', 'race', 'age', 'income']
# demographics_2024: ['person_id']
