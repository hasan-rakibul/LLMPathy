seed: 0
num_workers: 12
plm: roberta-base
# num_epochs: 30
# lr: 1.0e-5
# batch_size: 16
logging_dir: ./logs
# fc_arch: [768, 512, 256, 1] # [out_transforer, out_pre_fusion, out_fusion, out]
# max_length: 512
freeze_plm: False
lr_find: False  # unfornately, it didn't work for me

# adamw_beta1: 0.9
# adamw_beta2: 0.98
# adamw_eps: 1.0e-6
# adamw_weight_decay: 0.1

# lr_scheduler_type: linear
# lr_scheduler_type: plateau
# linear_warmup: 0.06
# plateau_patience: 1
# plateau_factor: 0.1
# plateau_threshold: 0.001

# alpha: 5.59

label_column: empathy
# llm_column: llm_empathy
feature_to_tokenise: ["essay"]
# feature_to_tokenise: ["demographic_essay"]
# extra_columns_to_keep: ["article_id"]
extra_columns_to_keep: []

### extra column in train only
# extra_columns_to_keep_train: []
extra_columns_to_keep_train: ["llm_empathy"]


# demographics: ['gender', 'education', 'race', 'age', 'income']
# demographics_2024: ['person_id']
