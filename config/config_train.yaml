debug_mode: False # will run only 1 epoch of training with 1% of training data, saves logs in tmp/ directory

save_predictions_to_disk: False # True or False

main_label: y # y' - LLM-GEm; y - vanilla

train_data: [2024] # for y or y'
train_only_llm_data: [2023, 2022] # for y_llm

expt_name_postfix: ImprovedEarlyStop # postfix to be added on top of main_label, train_data and train_only_llm_data
# overwrite_logging_dir: logs/20241109_154046_y'(2024,2023,2022)-MultiHparam-Demog-10Epoch 
# resume_train_from_checkpoint: logs/20241023_205914_vanilla+2023LLM/lightning_logs/version_17110004/checkpoints/epoch=5-step=378.ckpt

# finetune_from_checkpoint: logs/20241114_153020_y'(2024,2023,2022)-ImprovedEarlyStop/seed_42/lightning_logs/version_18324709/checkpoints/epoch=10-step=2849.ckpt 
# finetune_lr: 3.0e-6

enable_early_stopping: True

num_epochs: 20

#### Hyperparameters to search
# lrs: [1.0e-5, 2.0e-5, 3.0e-5]
# batch_sizes: [16, 32]
# alphas: [0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0]

#### Selected hyperparameters
lrs: [3.0e-5]
batch_sizes: [16]
alphas: [4.5]

logging_dir: ./logs
freeze_plm: False
lr_find: False  # it didn't work for me

adamw_beta1: 0.9
adamw_beta2: 0.98
adamw_eps: 1.0e-06
adamw_weight_decay: 0.1

lr_scheduler_type: linear # False, linear, polynomial, plateau

warmup_ratio: 0.06
# num_warmup_steps: 150

# plateau_patience: 1
# plateau_factor: 0.05
# plateau_threshold: 1.0e-4
